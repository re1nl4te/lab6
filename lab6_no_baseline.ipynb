{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5970d1d-c4ae-4ac2-ad29-3865b897237d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in c:\\users\\sasin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.11.1 in c:\\users\\sasin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\sasin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations) (1.11.3)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in c:\\users\\sasin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations) (0.22.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\sasin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations) (6.0.1)\n",
      "Requirement already satisfied: qudida>=0.0.4 in c:\\users\\sasin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in c:\\users\\sasin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from albumentations) (4.8.1.78)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\sasin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qudida>=0.0.4->albumentations) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\sasin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from qudida>=0.0.4->albumentations) (4.8.0)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\sasin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (3.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\users\\sasin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (10.0.1)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\users\\sasin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\sasin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (2023.12.9)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\sasin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (23.1)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in c:\\users\\sasin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (0.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\sasin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sasin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "!pip install albumentations\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A\n",
    "\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "!pip install -q segmentation-models-pytorch\n",
    "!pip install -q torchsummary\n",
    "\n",
    "from torchsummary import summary\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c508c476-7cad-461e-8610-8fff1c02a4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16.0+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f1ee68b-d347-4e35-b15f-2f77a768a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "IMAGE_PATH = 'dataset/semantic_drone_dataset/original_images/'\n",
    "MASK_PATH = 'dataset/semantic_drone_dataset/label_images_semantic/'\n",
    "\n",
    "batch_size = 8\n",
    "n_classes = 23\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "349c6b17-7fae-4d4d-b304-fcc567e023a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dataset/semantic_drone_dataset/original_images/'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0970282-b563-41b6-bd62-fc0c148c85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 23 \n",
    "\n",
    "def create_df():\n",
    "    name = []\n",
    "    for dirname, _, filenames in os.walk(IMAGE_PATH):\n",
    "        for filename in filenames:\n",
    "            name.append(filename.split('.')[0])\n",
    "    \n",
    "    return pd.DataFrame({'id': name}, index = np.arange(0, len(name)))\n",
    "\n",
    "def get_mask(name):\n",
    "    mask_path = MASK_PATH + name\n",
    "    mask = Image.open(mask_path).convert('RGB')\n",
    "    mask = np.array(mask)\n",
    "    mask = np.argmax(mask, axis=2)\n",
    "    mask = np.expand_dims(mask, axis=2)\n",
    "    mask = torch.Tensor(mask).long()\n",
    "    return mask\n",
    "\n",
    "def get_transforms():\n",
    "    transform = {\n",
    "        'train': A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.Transpose(p=0.5),\n",
    "            A.Normalize(),\n",
    "            ToTensor(),\n",
    "        ]),\n",
    "        'valid': A.Compose([\n",
    "            A.Normalize(),\n",
    "            ToTensor(),\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9be231eb-e25f-4c29-b841-ad1015d770d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images:  400\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = create_df()\n",
    "print('Total Images: ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "77ca7256-eeda-4e1c-ab68-544c537f345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DroneDataset(Dataset):\n",
    "    def __init__(self, df, image_path, mask_path, transform=None):\n",
    "        self.df = df\n",
    "        self.image_path = image_path\n",
    "        self.mask_path = mask_path\n",
    "        self.transform = transform\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        name = self.df['name'][index]\n",
    "        image_path = self.image_path + name\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        mask = get_mask(name)\n",
    "      \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=np.array(image), mask=np.array(mask))\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ccea8c79-8bfc-406c-a833-3db2346d79e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "  \n",
    "    train_dataset = DroneDataset(train_df, IMAGE_PATH, MASK_PATH, get_transforms()['train'])\n",
    "    valid_dataset = DroneDataset(valid_df, IMAGE_PATH, MASK_PATH, get_transforms()['valid'])\n",
    "  \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)#до этого нумлодерс было 4\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0)#до этого нумлодерс было 4\n",
    "  \n",
    "    model = smp.Unet('resnet34', encoder_weights='imagenet', classes=n_classes).to(device)\n",
    "  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "  \n",
    "    num_epochs = 10\n",
    "  \n",
    "    train_loss_history = []\n",
    "    valid_loss_history = []\n",
    "    mIoU_history = []\n",
    "  \n",
    "    best_mIoU = 0.0\n",
    "  \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        mIoU = 0.0\n",
    "      \n",
    "        model.train()\n",
    "        for images, masks in tqdm(train_loader):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "          \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks.squeeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "          \n",
    "            train_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, masks in tqdm(valid_loader):\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "              \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks.squeeze(1))\n",
    "                valid_loss += loss.item() * images.size(0)\n",
    "              \n",
    "                outputs = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
    "                masks = masks.squeeze(1).detach().cpu().numpy()\n",
    "                mIoU += compute_iou(outputs, masks)\n",
    "          \n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "        mIoU = mIoU / len(valid_loader.dataset)\n",
    "        \n",
    "        train_loss_history.append(train_loss)\n",
    "        valid_loss_history.append(valid_loss)\n",
    "        mIoU_history.append(mIoU)\n",
    "      \n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.6f}, Valid Loss: {valid_loss:.6f}, mIoU: {mIoU:.6f}')\n",
    "      \n",
    "        if mIoU > best_mIoU:\n",
    "            best_mIoU = mIoU\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "      \n",
    "        scheduler.step(valid_loss)\n",
    "      \n",
    "    return train_loss_history, valid_loss_history, mIoU_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4b6f5e8-c57c-427b-86e8-a1250cef5d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab17d4f563944bc5b44964ef34b96327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loss_history, valid_loss_history, mIoU_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[54], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m mIoU \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m     31\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     32\u001b[0m     masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\notebook.py:249\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    248\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[53], line 12\u001b[0m, in \u001b[0;36mDroneDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m---> 12\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[index]\n\u001b[0;32m     13\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_path \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m     14\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3896\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3898\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'name'"
     ]
    }
   ],
   "source": [
    "train_loss_history, valid_loss_history, mIoU_history = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e5822-70db-4231-9e92-4759efb20923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d0ce91-5cf3-453a-830d-e43b9c962bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b848e0a6-6c3b-449c-9674-a6a396ec7cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15903a44-5650-4312-a440-05e6f2417b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa1ee4b-3e17-4465-a246-ae8b19a5eb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce163802-b202-45a4-9133-9cd1c05a9b79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
